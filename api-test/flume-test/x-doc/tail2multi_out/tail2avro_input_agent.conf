# 功能: flume1负责监控本地文件,克隆两份输入流,分别传输给flume2,flume3,   exec->avro 
# flume2到指定4141端口接收数据,然后上传到hdfs  avro->hdfs 
# flume3到指定4142端口接收数据,然后保存在本地 avro->file_roll

# nohup bin/flume-ng agent  --conf conf/ --name tail2avro_input_agent --conf-file agent/tail2multi_out/tail2avro_input_agent.conf 2>&1 &

#  step1: 声明三大组件 (r1->c1->k1, r1->c2->k2)
tail2avro_input_agent.sources = r1
tail2avro_input_agent.sinks = k1 k2
tail2avro_input_agent.channels = c1 c2
# 将数据流复制给多个channel
tail2avro_input_agent.sources.r1.selector.type = replicating


# step2: 定义r1,执行shell命令,监控指定文件
tail2avro_input_agent.sources.r1.type = exec
tail2avro_input_agent.sources.r1.command = tail -F /Users/huhao/software/virtual_space/running_jars/call-mocker/logs/call.log
tail2avro_input_agent.sources.r1.shell = /bin/bash -c


# step3-1: 定义k1, flume本机为hadoop102,往hadoop103:4141端口发送数据
tail2avro_input_agent.sinks.k1.type = avro
tail2avro_input_agent.sinks.k1.hostname = localhost
tail2avro_input_agent.sinks.k1.port = 4141

# step3-2: 定义k2, flume本机为hadoop102,往hadoop104:4142端口发送数据
tail2avro_input_agent.sinks.k2.type = avro
tail2avro_input_agent.sinks.k2.hostname = localhost
tail2avro_input_agent.sinks.k2.port = 4142


#  step4-1: 定义channel组件(基于内存构建事件队列),通道负载最大并发量1000个Event事件,100个事务
tail2avro_input_agent.channels.c1.type = memory
tail2avro_input_agent.channels.c1.capacity = 1000
tail2avro_input_agent.channels.c1.transactionCapacity = 100

#  step4-2: 定义channel组件(基于内存构建事件队列),通道负载最大并发量1000个Event事件,100个事务
tail2avro_input_agent.channels.c2.type = memory
tail2avro_input_agent.channels.c2.capacity = 1000
tail2avro_input_agent.channels.c2.transactionCapacity = 100


# step5: 组装
tail2avro_input_agent.sources.r1.channels = c1 c2
tail2avro_input_agent.sinks.k1.channel = c1
tail2avro_input_agent.sinks.k2.channel = c2